{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pozir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re                                  \n",
    "import string                              \n",
    "from nltk.corpus import stopwords          \n",
    "from nltk.stem import PorterStemmer        \n",
    "from nltk.tokenize import TweetTokenizer \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_tweet(text):\n",
    "    '''\n",
    "    A function that does tokenizing, lowercasing, removing stop words and \n",
    "    punctuation and stems a string.\n",
    "    '''\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    stemmer = PorterStemmer() \n",
    "\n",
    "    # remove stock market tickers like $GE and old style retweet text \"RT\"  \n",
    "    string2 = re.sub(r'\\$\\w*', '', text)\n",
    "    string2 = re.sub(r'^RT[\\s]+', '', string2)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    string2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', string2)\n",
    "\n",
    "    # remove # sign\n",
    "    string2 = re.sub(r'#', '', string2)\n",
    "\n",
    "    # instantiate tokenizer class\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                                   reduce_len=True)\n",
    "\n",
    "    # tokenize tweets\n",
    "    string_tokens = tokenizer.tokenize(string2)\n",
    "\n",
    "    string_clean = []\n",
    "\n",
    "    # remove stop words and punctuation\n",
    "    for word in string_tokens:\n",
    "        if (word not in stopwords_english and\n",
    "            word not in string.punctuation):\n",
    "            string_clean.append(word)\n",
    "\n",
    "    # stemming the tokens\n",
    "    string_stem = []\n",
    "\n",
    "    for word in string_clean:\n",
    "        stem_word = stemmer.stem(word) \n",
    "        string_stem.append(stem_word) \n",
    "\n",
    "    return string_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dict(texts, labels):\n",
    "    \"\"\"count_dict.\n",
    "    Input:\n",
    "    texts: a list of tweets\n",
    "    labels: an m x 1 array with the sentiment label of each tweet\n",
    "    (either 0 or 1)\n",
    "    Output:\n",
    "    freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "    frequency\n",
    "    \"\"\"\n",
    "    labels = np.squeeze(labels).tolist()\n",
    "    freq_dict = {}\n",
    "    for tweet, y in zip(texts, labels):\n",
    "        for w in prepro_tweet(tweet):\n",
    "            k = (w, y)\n",
    "            if k in freq_dict:\n",
    "                freq_dict[k] += 1\n",
    "            else:\n",
    "                freq_dict[k] = 1\n",
    "\n",
    "    return freq_dict     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_class(labels, sizes):\n",
    "    '''\n",
    "    A function that visualizes the proportion of the predicted classes;\n",
    "    labels: a list of the class names\n",
    "    sizes: a list ofclasses counts\n",
    "    '''\n",
    "    # Declare a figure with a custom size\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "    # Declare pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "            shadow=True, startangle=90)\n",
    "\n",
    "    # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    plt.axis('equal')\n",
    "    # Display the chart\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_words(list_of_words, dict_freq):\n",
    "    '''\n",
    "    A function that visualizes how often given words apper in positive and negative \n",
    "    tweets. The plot is in the logarithmic scale to take into account the wide \n",
    "    discrepancies between the raw counts. The red line marks the boundary between \n",
    "    positive and negative areas. Words close to the red line can be classified as \n",
    "    neutral.\n",
    "    list_of_words: the list of the preprocessed words from the tweets;\n",
    "    dict_freq: a dictionary of the word frequencies per each class;\n",
    "    '''\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    data = []\n",
    "    \n",
    "    for word in list_of_words:\n",
    "        if (word, 1.0) in dict_freq:\n",
    "            pos = dict_freq[(word, 1.0)]\n",
    "        if (word, 0.0) in dict_freq:\n",
    "            neg = dict_freq[(word, 0.0)]\n",
    "        data.append([word, pos, neg])\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize = (8, 8))\n",
    "\n",
    "    # convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\n",
    "    x = np.log([x[1] + 1 for x in data])  \n",
    "\n",
    "    # do the same for the negative counts\n",
    "    y = np.log([x[2] + 1 for x in data]) \n",
    "\n",
    "    # Plot a dot for each pair of words\n",
    "    ax.scatter(x, y)  \n",
    "\n",
    "    # assign axis labels\n",
    "    plt.xlabel(\"Log Positive count\")\n",
    "    plt.ylabel(\"Log Negative count\")\n",
    "\n",
    "    # Add the word as the label at the same position as you added the points just before\n",
    "    for i in range(0, len(data)):\n",
    "        ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n",
    "\n",
    "    ax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\n",
    "    plt.show()\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    '''\n",
    "    Input:\n",
    "        z: is the input (can be a scalar or an array)\n",
    "    Output:\n",
    "        h: the sigmoid of z\n",
    "    '''      \n",
    "    h = 1/(1+ 2.718281828459045**(-z))\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    This function takes in a single tweet, processes it using the imported prepro_tweet() function and \n",
    "    saves the list of tweet words, loop through each word in the list of processed words and for each \n",
    "    word, check the frequency dictionary for the count when that word has a positive '1' label.\n",
    "    Input: \n",
    "        tweet: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = prepro_tweet(tweet)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1 \n",
    "    \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        x[0,1] += freqs.get((word, 1.0), 0)\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        x[0,2] += freqs.get((word, 0.0), 0)\n",
    "        \n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: your final weight vector\n",
    "    '''\n",
    "    m = len(x)\n",
    "    iter_times = []\n",
    "    costs = []\n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        iter_times.append(i)\n",
    "        # get z, the dot product of x and theta\n",
    "        z = x.dot(theta)\n",
    "        \n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # calculate the cost function\n",
    "        J = (-1/m)*((y.T.dot(np.log(h)))+((1-y).T.dot(np.log(1-h))))\n",
    "        #print(type(J))\n",
    "        #print(J)\n",
    "        costs.append(J[0][0])\n",
    "        \n",
    "        # update the weights theta\n",
    "        theta = theta-alpha/m*(x.T.dot(h - y))\n",
    "    plt.plot(iter_times, costs)\n",
    "    plt.show()    \n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    '''\n",
    "    Given a tweet the function processes it, extracts the features, applies the model's learned weights on \n",
    "    the features to get the logits, applies the sigmoid to the logits to get the prediction\n",
    "    (a value between 0 and 1). ð‘¦ð‘ð‘Ÿð‘’ð‘‘=ð‘ ð‘–ð‘”ð‘šð‘œð‘–ð‘‘(ð±â‹…ðœƒ)\n",
    "    Input: \n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output: \n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    \n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet, freqs)\n",
    "    \n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(x.dot(theta))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(test_x, test_y, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of tweets\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: weight vector of dimension (3, 1)\n",
    "    Output: \n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"   \n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1.0)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0.0)\n",
    "            \n",
    "    y_hat = np.array(y_hat)\n",
    "    y_hat.resize((len(y_hat), 1))\n",
    "    accuracy = ((y_hat == test_y).sum())/len(test_y)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_pred(text, freqs, theta):\n",
    "    print(prepro_tweet(text))\n",
    "    y_hat = predict_tweet(text, freqs, theta)\n",
    "    print(y_hat)\n",
    "    if y_hat > 0.5:\n",
    "        print('Positive sentiment')\n",
    "    else:\n",
    "        print('Negative sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misClass(test_x, test_y, freqs, theta):\n",
    "    '''\n",
    "    Function that returns the tweets that were misclassified.\n",
    "    '''\n",
    "    print('Label Predicted Tweet')\n",
    "    for x,y in zip(test_x,test_y):\n",
    "        y_hat = predict_tweet(x, freqs, theta)\n",
    "            \n",
    "        if np.abs(y - (y_hat > 0.5)) > 0:\n",
    "            print('THE TWEET IS:', x)\n",
    "            print('THE PROCESSED TWEET IS:', prepro_tweet(x))\n",
    "            print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(prepro_tweet(x)).encode('ascii', 'ignore')))\n",
    "            print('********************************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Sentiment_Analysis_LogReg'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior.\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation. \n",
    "    '''\n",
    "    N = len(train_y)\n",
    "    N_pos = sum(train_y)/N\n",
    "    N_neg = 1 - N_pos\n",
    "    logprior = np.log(N_pos) - np.log(N_neg)\n",
    "    # create the vocabulary of the unique words\n",
    "    vocab = set([i[0] for i in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "    # the number of words in the vocabulary per class\n",
    "    num_words_pos  = 0\n",
    "    num_words_neg = 0\n",
    "    \n",
    "    for w in vocab:\n",
    "        \n",
    "        num_words_pos += freqs.get((w, 1), 0)\n",
    "        num_words_neg += freqs.get((w, 0), 0)\n",
    "    \n",
    "    loglikelihood = {}\n",
    "    \n",
    "    for w in vocab:\n",
    "        \n",
    "        aux_pos = (freqs.get((w, 1), 0) + 1)/(num_words_pos + V)\n",
    "        aux_neg = (freqs.get((w, 0), 0) + 1)/(num_words_neg + V)\n",
    "        loglikelihood[w] = np.log(aux_pos/aux_neg)\n",
    "        \n",
    "    return logprior, loglikelihood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(text, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    tok_list = prepro_tweet(text)\n",
    "    log_like_sum = 0\n",
    "    for i in tok_list:\n",
    "        log_like_sum += loglikelihood.get(i, 0)\n",
    "    return logprior + log_like_sum  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of tweets\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = []\n",
    "    for i in test_x:\n",
    "        score = naive_bayes_predict(i, logprior, loglikelihood)\n",
    "        if score > 0:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    accuracy_score = 1 - (sum(abs(predictions-test_y)))/(len(test_y))\n",
    "    \n",
    "    return accuracy_score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(freqs, word):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary containing the words\n",
    "        word: string to lookup\n",
    "\n",
    "    Output: a dictionary with keys 'positive', 'negative', and 'ratio'.\n",
    "        Example: {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "    '''\n",
    "    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n",
    "    pos_neg_ratio['positive'] = freqs.get((word, 1), 0)\n",
    "    pos_neg_ratio['negative'] = freqs.get((word, 0), 1)\n",
    "    pos_neg_ratio['ratio'] = (pos_neg_ratio['positive'] + 1)/(pos_neg_ratio['negative'] + 1)\n",
    "    \n",
    "    \n",
    "    return pos_neg_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_by_threshold(freqs, label, threshold):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary of words\n",
    "        label: 1 for positive, 0 for negative\n",
    "        threshold: ratio that will be used as the cutoff for including a word in the returned dictionary\n",
    "    Output:\n",
    "        word_set: dictionary containing the word and information on its positive count, negative count, and ratio of positive to negative counts.\n",
    "        example of a key value pair:\n",
    "        {'happi':\n",
    "            {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "        }\n",
    "    '''\n",
    "    word_list = {}\n",
    "\n",
    "    for key in freqs.keys():\n",
    "        word, _ = key\n",
    "\n",
    "        # get the positive/negative ratio for a word\n",
    "        pos_neg_ratio = get_ratio(freqs, word)['ratio']\n",
    "\n",
    "        # if the label is 1 and the ratio is greater than or equal to the threshold...\n",
    "        if label == 1 and pos_neg_ratio >= threshold :\n",
    "\n",
    "            # Add the pos_neg_ratio to the dictionary\n",
    "            word_list[word] = {'positive':get_ratio(freqs, word)['positive'] , \n",
    "                               'negative': get_ratio(freqs, word)['negative'],\n",
    "                               'ratio': pos_neg_ratio}\n",
    "\n",
    "        # If the label is 0 and the pos_neg_ratio is less than or equal to the threshold...\n",
    "        elif label == 0 and pos_neg_ratio <= threshold:\n",
    "\n",
    "            # Add the pos_neg_ratio to the dictionary\n",
    "            word_list[word] = {'positive':get_ratio(freqs, word)['positive'] , \n",
    "                               'negative': get_ratio(freqs, word)['negative'],\n",
    "                               'ratio': pos_neg_ratio}\n",
    "\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missclassNb(test_x, test_y, logprior, loglikelihood):\n",
    "    print('Truth Predicted Tweet')\n",
    "    for x, y in zip(test_x, test_y):\n",
    "        y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
    "        if y != (np.sign(y_hat) > 0):\n",
    "            print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "                prepro_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_likelihood(freqs, train_y, train_x):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior.\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation. \n",
    "    '''\n",
    "    \n",
    "    # create the vocabulary of the unique words\n",
    "    vocab = set([i[0] for i in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "    # the number of words in the vocabulary per class\n",
    "    num_words_pos  = 0\n",
    "    num_words_neg = 0\n",
    "    \n",
    "    for w in vocab:\n",
    "        \n",
    "        num_words_pos += freqs.get((w, 1), 0)\n",
    "        num_words_neg += freqs.get((w, 0), 0)\n",
    "    \n",
    "    loglikelihood = {}\n",
    "    \n",
    "    for w in vocab:\n",
    "        \n",
    "        aux_pos = (freqs.get((w, 1), 0) + 1)/(num_words_pos + V)\n",
    "        aux_neg = (freqs.get((w, 0), 0) + 1)/(num_words_neg + V)\n",
    "        loglikelihood[(w, 1)] = aux_pos\n",
    "        loglikelihood[(w, 0)] = aux_neg\n",
    "    \n",
    "    pos = []\n",
    "    neg = []\n",
    "    for i in train_x:\n",
    "        \n",
    "        token_list = prepro_tweet(i)\n",
    "        positive = sum([np.log(loglikelihood[(t, 1)]) for t in token_list])\n",
    "        pos.append(positive)\n",
    "        negative = sum([np.log(loglikelihood[(t, 0)]) for t in token_list])\n",
    "        neg.append(negative)\n",
    "\n",
    "    d = {'positive': pos, 'negative': neg, 'sentiment': list(train_y)}\n",
    "    bayes_feat = pd.DataFrame(d)\n",
    "    \n",
    "        \n",
    "    return bayes_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_ellipse(x, y, ax, n_std=3.0, facecolor='none', **kwargs):\n",
    "    \"\"\"\n",
    "    Create a plot of the covariance confidence ellipse of *x* and *y*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : array-like, shape (n, )\n",
    "        Input data.\n",
    "\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes object to draw the ellipse into.\n",
    "\n",
    "    n_std : float\n",
    "        The number of standard deviations to determine the ellipse's radiuses.\n",
    "\n",
    "    **kwargs\n",
    "        Forwarded to `~matplotlib.patches.Ellipse`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.patches.Ellipse\n",
    "    \"\"\"\n",
    "    if x.size != y.size:\n",
    "        raise ValueError(\"x and y must be the same size\")\n",
    "\n",
    "    cov = np.cov(x, y)\n",
    "    pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])\n",
    "    # Using a special case to obtain the eigenvalues of this\n",
    "    # two-dimensionl dataset.\n",
    "    ell_radius_x = np.sqrt(1 + pearson)\n",
    "    ell_radius_y = np.sqrt(1 - pearson)\n",
    "    ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2,\n",
    "                      facecolor=facecolor, **kwargs)\n",
    "\n",
    "    # Calculating the stdandard deviation of x from\n",
    "    # the squareroot of the variance and multiplying\n",
    "    # with the given number of standard deviations.\n",
    "    scale_x = np.sqrt(cov[0, 0]) * n_std\n",
    "    mean_x = np.mean(x)\n",
    "\n",
    "    # calculating the stdandard deviation of y ...\n",
    "    scale_y = np.sqrt(cov[1, 1]) * n_std\n",
    "    mean_y = np.mean(y)\n",
    "\n",
    "    transf = transforms.Affine2D() \\\n",
    "        .rotate_deg(45) \\\n",
    "        .scale(scale_x, scale_y) \\\n",
    "        .translate(mean_x, mean_y)\n",
    "\n",
    "    ellipse.set_transform(transf + ax.transData)\n",
    "    return ax.add_patch(ellipse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
