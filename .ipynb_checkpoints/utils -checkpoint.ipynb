{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pozir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re                                  \n",
    "import string                              \n",
    "from nltk.corpus import stopwords          \n",
    "from nltk.stem import PorterStemmer        \n",
    "from nltk.tokenize import TweetTokenizer \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_tweet(text):\n",
    "    '''\n",
    "    A function that does tokenizing, lowercasing, removing stop words and \n",
    "    punctuation and stems a string.\n",
    "    '''\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    stemmer = PorterStemmer() \n",
    "\n",
    "    # remove stock market tickers like $GE and old style retweet text \"RT\"  \n",
    "    string2 = re.sub(r'\\$\\w*', '', text)\n",
    "    string2 = re.sub(r'^RT[\\s]+', '', string2)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    string2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', string2)\n",
    "\n",
    "    # remove # sign\n",
    "    string2 = re.sub(r'#', '', string2)\n",
    "\n",
    "    # instantiate tokenizer class\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                                   reduce_len=True)\n",
    "\n",
    "    # tokenize tweets\n",
    "    string_tokens = tokenizer.tokenize(string2)\n",
    "\n",
    "    string_clean = []\n",
    "\n",
    "    # remove stop words and punctuation\n",
    "    for word in string_tokens:\n",
    "        if (word not in stopwords_english and\n",
    "            word not in string.punctuation):\n",
    "            string_clean.append(word)\n",
    "\n",
    "    # stemming the tokens\n",
    "    string_stem = []\n",
    "\n",
    "    for word in string_clean:\n",
    "        stem_word = stemmer.stem(word) \n",
    "        string_stem.append(stem_word) \n",
    "\n",
    "    return string_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dict(texts, labels):\n",
    "    \"\"\"count_dict.\n",
    "    Input:\n",
    "    texts: a list of tweets\n",
    "    labels: an m x 1 array with the sentiment label of each tweet\n",
    "    (either 0 or 1)\n",
    "    Output:\n",
    "    freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "    frequency\n",
    "    \"\"\"\n",
    "    labels = np.squeeze(labels).tolist()\n",
    "    freq_dict = {}\n",
    "    for tweet, y in zip(texts, labels):\n",
    "        for w in prepro_tweet(tweet):\n",
    "            k = (w, y)\n",
    "            if k in freq_dict:\n",
    "                freq_dict[k] += 1\n",
    "            else:\n",
    "                freq_dict[k] = 1\n",
    "\n",
    "    return freq_dict     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_class(labels, sizes):\n",
    "    '''\n",
    "    A function that visualizes the proportion of the predicted classes;\n",
    "    labels: a list of the class names\n",
    "    sizes: a list ofclasses counts\n",
    "    '''\n",
    "    # Declare a figure with a custom size\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "    # Declare pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "            shadow=True, startangle=90)\n",
    "\n",
    "    # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    plt.axis('equal')\n",
    "    # Display the chart\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_words(list_of_words, dict_freq):\n",
    "    '''\n",
    "    A function that visualizes how often given words apper in positive and negative \n",
    "    tweets. The plot is in the logarithmic scale to take into account the wide \n",
    "    discrepancies between the raw counts. The red line marks the boundary between \n",
    "    positive and negative areas. Words close to the red line can be classified as \n",
    "    neutral.\n",
    "    list_of_words: the list of the preprocessed words from the tweets;\n",
    "    dict_freq: a dictionary of the word frequencies per each class;\n",
    "    '''\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    data = []\n",
    "    \n",
    "    for word in list_of_words:\n",
    "        if (word, 1.0) in dict_freq:\n",
    "            pos = dict_freq[(word, 1.0)]\n",
    "        if (word, 0.0) in dict_freq:\n",
    "            neg = dict_freq[(word, 0.0)]\n",
    "        data.append([word, pos, neg])\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize = (8, 8))\n",
    "\n",
    "    # convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\n",
    "    x = np.log([x[1] + 1 for x in data])  \n",
    "\n",
    "    # do the same for the negative counts\n",
    "    y = np.log([x[2] + 1 for x in data]) \n",
    "\n",
    "    # Plot a dot for each pair of words\n",
    "    ax.scatter(x, y)  \n",
    "\n",
    "    # assign axis labels\n",
    "    plt.xlabel(\"Log Positive count\")\n",
    "    plt.ylabel(\"Log Negative count\")\n",
    "\n",
    "    # Add the word as the label at the same position as you added the points just before\n",
    "    for i in range(0, len(data)):\n",
    "        ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n",
    "\n",
    "    ax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\n",
    "    plt.show()\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    '''\n",
    "    Input:\n",
    "        z: is the input (can be a scalar or an array)\n",
    "    Output:\n",
    "        h: the sigmoid of z\n",
    "    '''      \n",
    "    h = 1/(1+ 2.718281828459045**(-z))\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    This function takes in a single tweet, processes it using the imported prepro_tweet() function and \n",
    "    saves the list of tweet words, loop through each word in the list of processed words and for each \n",
    "    word, check the frequency dictionary for the count when that word has a positive '1' label.\n",
    "    Input: \n",
    "        tweet: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = prepro_tweet(tweet)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1 \n",
    "    \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        x[0,1] += freqs.get((word, 1.0), 0)\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        x[0,2] += freqs.get((word, 0.0), 0)\n",
    "        \n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: your final weight vector\n",
    "    '''\n",
    "    m = len(x)\n",
    "    iter_times = []\n",
    "    costs = []\n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        iter_times.append(i)\n",
    "        # get z, the dot product of x and theta\n",
    "        z = x.dot(theta)\n",
    "        \n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # calculate the cost function\n",
    "        J = (-1/m)*((y.T.dot(np.log(h)))+((1-y).T.dot(np.log(1-h))))\n",
    "        #print(type(J))\n",
    "        #print(J)\n",
    "        costs.append(J[0][0])\n",
    "        \n",
    "        # update the weights theta\n",
    "        theta = theta-alpha/m*(x.T.dot(h - y))\n",
    "    plt.plot(iter_times, costs)\n",
    "    plt.show()    \n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    '''\n",
    "    Given a tweet the function processes it, extracts the features, applies the model's learned weights on \n",
    "    the features to get the logits, applies the sigmoid to the logits to get the prediction\n",
    "    (a value between 0 and 1). 𝑦𝑝𝑟𝑒𝑑=𝑠𝑖𝑔𝑚𝑜𝑖𝑑(𝐱⋅𝜃)\n",
    "    Input: \n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output: \n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    \n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet, freqs)\n",
    "    \n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(x.dot(theta))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(test_x, test_y, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of tweets\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: weight vector of dimension (3, 1)\n",
    "    Output: \n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"   \n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1.0)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0.0)\n",
    "            \n",
    "    y_hat = np.array(y_hat)\n",
    "    y_hat.resize((len(y_hat), 1))\n",
    "    accuracy = ((y_hat == test_y).sum())/len(test_y)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_pred(text, freqs, theta):\n",
    "    print(prepro_tweet(text))\n",
    "    y_hat = predict_tweet(text, freqs, theta)\n",
    "    print(y_hat)\n",
    "    if y_hat > 0.5:\n",
    "        print('Positive sentiment')\n",
    "    else:\n",
    "        print('Negative sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misClass(test_x, test_y, freqs, theta):\n",
    "    '''\n",
    "    Function that returns the tweets that were misclassified.\n",
    "    '''\n",
    "    print('Label Predicted Tweet')\n",
    "    for x,y in zip(test_x,test_y):\n",
    "        y_hat = predict_tweet(x, freqs, theta)\n",
    "            \n",
    "        if np.abs(y - (y_hat > 0.5)) > 0:\n",
    "            print('THE TWEET IS:', x)\n",
    "            print('THE PROCESSED TWEET IS:', prepro_tweet(x))\n",
    "            print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(prepro_tweet(x)).encode('ascii', 'ignore')))\n",
    "            print('********************************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
